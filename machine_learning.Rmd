---
title: "Machine Learning Prediction Project"
author: "Paul Phan"
date: "August 16, 2017"
output: html_document
---

#Executive Summary
The goal of this project is to predict the manner (classe variable) in which the 6 participants did the exercise.

This report will first load the training and test data set into two separate data frames and then cleanse the data by removing any columns containing NAs that are not relevant or useful for prediction. Once the datasets are cleanse it will perform feature selection to ensure the most relevant variables are used for modelling.

Prior to model selection the training dataset will be furthur partitioned into 2 parts with the 1st part containing 70% of the observations used for model selection and the 2nd part containing 30% of the observations reserved for out-of-sample error validation. Finally when a model is selected and tested the report will predict on the 20 test cases in the testing dataset.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("AppliedPredictiveModeling")
#install.packages("rattle")
#install.packages("https://cran.r-project.org/bin/windows/contrib/3.3/RGtk2_2.20.31.zip", repos=NULL)
#install.packages("RGtk2")
#install.packages("rpart.plot")
#install.packages("RCurl")

#Libraries referenced
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(randomForest)
library(RCurl)
library(ggplot2)
```

#Load Data Section
Loading training and testing data:

    1. training = 19622 observations and 160 variables
    2. testing = 20 observations and 160 variables
```{r data_load,cache=TRUE}
setwd("~/Learning/Coursera/08 Machine Learning/Week 4/Course Project")

    if(!dir.exists("data")){
      dir.create("data")
    }

    #*** Training Data ***
    file_name<-"./Data/pml-training.csv"
    if(!file.exists("./Data/pml-training.csv")){#Download training data
      fileUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
      download.file(fileUrl,file_name)
    }
    if(!exists("training")){#Load training csv file into dataframe and replace null values with NA
      training<- read.csv(file_name, na.strings=c("NA",""), header=TRUE)
    }

    #*** Testing Data ***
    file_name<-"./Data/pml-testing.csv"
    if(!file.exists(file_name)){#Download testing data
      fileUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
      download.file(fileUrl,file_name)
    }
    if(!exists("testing")){#Load testing csv file into dataframe and replace null values with NA
      testing<- read.csv(file_name, na.strings=c("NA",""), header=TRUE)
    }
```

#Clean Data Section
Cleaning training and testing data:

    1. training = 19622 observations and 60 variables (100 variables containing NAs removed)
    2. testing = 20 observations and 60 variables (100 variables containing NAs removed)
```{r data_clean}
#Clean training by removing all columns containing at least one NA value (100 variables removed)
training_nona<-training[,colSums(is.na(training))==0] 

#Clean testing by removing all columns containing at least one NA value
testing_nona<-testing[,colSums(is.na(testing))==0]
```

#Feature Selection Section
Selecting variables for the model:

    1. removed first 7 variables from the traing data set as they are time and factor variables not relevant for the model
    2. removed 36 less relevant variables for better prediction
```{r data_feature}
#removing 5 not relevant variables and 36 less relevant variables
training_nona<-training_nona[,-c(1:7,12:20,25:33,38:46,51:59)] 
#View(training_nona)
testing_nona<-testing_nona[,-c(1:7,12:20,25:33,38:46,51:59)] 
```

#Data Slicing Section
Spliting the cleansed training data into a training set and a validation set for better algorithm performance.

Further slicing the 19622 observations into 20 folds of approximately 687 observations in order for the algorithms to run in a reasonable timeframe. Using the small sample size for model selection and in-sample error then the larger validation data set to test the model and get our out-of-sample error.
```{r data_slice}
#Partitioning cleansed training data into training set (70%) and validation set (30%)
training_sample<-createDataPartition(y=training_nona$classe,p=0.7,list=FALSE)
training_set<-training_nona[training_sample, ]
validation_set<-training_nona[-training_sample, ]

#Further slicing our training_set for use in model selection and in-sample error
folds<-createFolds(y=training_set$classe,k=20,list=TRUE,returnTrain=FALSE)
training_1<-training_set[folds$Fold01,]
training_10<-training_set[folds$Fold10,]
#View(training_1)
```

#Model Selection
Decided to use the top performing algorithms random forest and gradient boosting for model selections since their predictions are often very accurate (ref: week 3 slide 10 of 10)

    Result:
        1. Random forest = 82%
        2. Gradient boosting = 77%
        
    Selection:
        1. Random forest selected since it had the higer accuracy performance at 82%
```{r mod_select,cache=TRUE}
set.seed(1122)
mod_sel_rf<-train(classe~.,data=training_1,method='rf',prox=TRUE)
mod_sel_gbm<-train(classe~.,data=training_1,method="gbm",verbose=FALSE)
#save(mod_sel_rf, file='./mod_sel_rf.RData')
print(mod_sel_rf$results)
print(mod_sel_gbm$results)
```

#Cross-validation Section
In-Sample Error:

With the selected algorithm/model(random forest) we now use 10 fold cross validation on the training 1 subdata set to get our in-sample error

    Result = in-sample error 16%
    
```{r x-in_sample,cache=TRUE}
control<-trainControl('cv',10,savePredictions=TRUE) #setting 10 fold cross-validaton control
mod_fit_rf_in<-train(classe~.,data=training_1,method="rf",prox=TRUE,trControl=control) #in-sample model
print(mod_fit_rf_in)
```

Out-of-Sample Error:

With the selected algorithm/model(random forest) we now use 10 fold cross validation on the validation data set to get our out-of-sample error

    Result = out-of-sample error 3%  ***a significant improvement from our training data set
    
```{r x-out_sample,cache=TRUE}
control<-trainControl('cv',10,savePredictions=TRUE) #setting 10 fold cross-validaton control
mod_fit_rf_out<-train(classe~.,data=validation_set,method="rf",prox=TRUE,trControl=control) #out-sample model
print(mod_fit_rf_out)
```

#Plot of Out-of-Sample
Figure 1: Plot of observation centers for each prediction
```{r plot}
#Setup plot data for roll(belt,arm,dumbbell,forearm)
plot_data_1<-classCenter(validation_set[,c(1,4)],validation_set$classe,mod_fit_rf_out$finalModel$proximity)
plot_data_1<-as.data.frame(plot_data_1)
plot_data_1$classe<-rownames(plot_data_1)

#plot_data_2<-classCenter(validation_set[,c(5,8)],validation_set$classe,mod_fit_rf_out$finalModel$proximity)
#plot_data_2<-as.data.frame(plot_data_2)
#plot_data_2$classe<-rownames(plot_data_2)

#plot_data_3<-classCenter(validation_set[,c(9,12)],validation_set$classe,mod_fit_rf_out$finalModel$proximity)
#plot_data_3<-as.data.frame(plot_data_3)
#plot_data_3$classe<-rownames(plot_data_3)

#plot_data_4<-classCenter(validation_set[,c(13,16)],validation_set$classe,mod_fit_rf_out$finalModel$proximity)
#plot_data_4<-as.data.frame(plot_data_4)
#plot_data_4$classe<-rownames(plot_data_4)

#Plot for roll(belt,arm,dumbbell,forearm) by total acceleration per classe
par(mfrow=c(4,1))
plot_1<-qplot(roll_belt,total_accel_belt,col=classe,data=validation_set)
plot_1+geom_point(aes(x=roll_belt,y=total_accel_belt,col=classe),size=5,shape=4,data=plot_data_1)

#plot_2<-qplot(roll_arm,total_accel_arm,col=classe,data=validation_set)
#plot_2+geom_point(aes(x=roll_arm,y=total_accel_arm,col=classe),size=5,shape=4,data=plot_data_2)

#plot_3<-qplot(roll_dumbbell,total_accel_dumbbell,col=classe,data=validation_set)
#plot_3+geom_point(aes(x=roll_dumbbell,y=total_accel_dumbbell,col=classe),size=5,shape=4,data=plot_data_3)

#plot_4<-qplot(roll_forearm,total_accel_forearm,col=classe,data=validation_set)
#$plot_4+geom_point(aes(x=roll_forearm,y=total_accel_forearm,col=classe),size=5,shape=4,data=plot_data_4)
```


#Prediction Model Section
1. Test run on validation data set yielded 100% accuracy
```{r predict_validate}
pred<-predict(mod_fit_rf_out,validation_set)
validation_set$predRight<-pred==validation_set$classe
table(pred,validation_set$classe)

cmatrix_rf<-confusionMatrix(pred,validation_set$classe)
AccuracyResults<-data.frame(Model=c('Random Forest'),Accuracy=rbind(cmatrix_rf$overall[1]))
print(AccuracyResults)
```

2. Final run on testing data set also yielded 100% accuracy

In conclusion, using the random forest algorithm with the right selected features performed a very accurate prediction as mentioned in the lectures.  However, the algorithm does take a long time to run when using a large dataset with 10 fold validation, but the accuracy outweighs the performance and I was pleasantly surprised to see the validation accuracy was 100% as well as the final model prediction on the 20 observations was also 100%.
```{r predict_testing}
pred_test<-predict(mod_fit_rf_out,testing_nona)
results<-data.frame(problem_id=testing_nona$problem_id,predicted=pred_test)
testing_nona$classe<-pred_test
#print(results)

testing_nona$predRight<-pred_test==testing_nona$classe
table(pred_test,testing_nona$classe)

cmatrix_rf<-confusionMatrix(pred_test,testing_nona$classe)
AccuracyResults<-data.frame(Model=c('Random Forest'),Accuracy=rbind(cmatrix_rf$overall[1]))
print(AccuracyResults)
```

